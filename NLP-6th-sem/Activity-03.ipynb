{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99b669a",
   "metadata": {},
   "source": [
    "## GFG N-grams code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09b7370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import string \n",
    "import random \n",
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('reuters') \n",
    "from nltk.corpus import reuters \n",
    "from nltk import FreqDist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d967f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the reuters sentences \n",
    "sents  =reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d339af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8971bb46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['should',\n",
       " 'so',\n",
       " 'other',\n",
       " 'they',\n",
       " 'do',\n",
       " \"you're\",\n",
       " 'against',\n",
       " \"won't\",\n",
       " \"didn't\",\n",
       " 'she',\n",
       " 'had',\n",
       " 'by',\n",
       " \"couldn't\",\n",
       " \"don't\",\n",
       " 'our',\n",
       " \"shan't\",\n",
       " 'yours',\n",
       " 're',\n",
       " 'doing',\n",
       " 'there',\n",
       " 'has',\n",
       " 'very',\n",
       " 'during',\n",
       " 't',\n",
       " 'which',\n",
       " 'that',\n",
       " 'all',\n",
       " 'just',\n",
       " 'itself',\n",
       " 'until',\n",
       " \"aren't\",\n",
       " 'being',\n",
       " 'between',\n",
       " 'its',\n",
       " 'out',\n",
       " 'again',\n",
       " 'once',\n",
       " 'whom',\n",
       " 'below',\n",
       " 'who',\n",
       " 'of',\n",
       " 'above',\n",
       " 'hadn',\n",
       " 'theirs',\n",
       " 'ma',\n",
       " \"shouldn't\",\n",
       " \"that'll\",\n",
       " 'can',\n",
       " 'yourself',\n",
       " 'don',\n",
       " 'myself',\n",
       " 'as',\n",
       " 'too',\n",
       " 'not',\n",
       " 'aren',\n",
       " 'now',\n",
       " 'haven',\n",
       " 'it',\n",
       " 'what',\n",
       " 'himself',\n",
       " 'to',\n",
       " 'if',\n",
       " 'won',\n",
       " 'we',\n",
       " 'y',\n",
       " 'ourselves',\n",
       " 'a',\n",
       " 'no',\n",
       " 'about',\n",
       " 'be',\n",
       " 'more',\n",
       " 'ours',\n",
       " 'any',\n",
       " \"she's\",\n",
       " 'nor',\n",
       " 'will',\n",
       " 'because',\n",
       " 'was',\n",
       " 'having',\n",
       " 'but',\n",
       " 'than',\n",
       " 'is',\n",
       " \"should've\",\n",
       " 'you',\n",
       " 'their',\n",
       " 'down',\n",
       " 'on',\n",
       " 'where',\n",
       " 'my',\n",
       " 'themselves',\n",
       " 'into',\n",
       " \"mightn't\",\n",
       " 'am',\n",
       " 'needn',\n",
       " \"wasn't\",\n",
       " 'were',\n",
       " 'mustn',\n",
       " 'those',\n",
       " 'the',\n",
       " 'through',\n",
       " \"mustn't\",\n",
       " \"haven't\",\n",
       " \"it's\",\n",
       " 'her',\n",
       " 'and',\n",
       " 'up',\n",
       " \"needn't\",\n",
       " 'me',\n",
       " 'here',\n",
       " 'why',\n",
       " 'most',\n",
       " 'such',\n",
       " 'own',\n",
       " 'same',\n",
       " 'over',\n",
       " 'from',\n",
       " 'or',\n",
       " 'wouldn',\n",
       " 'herself',\n",
       " 'this',\n",
       " 'doesn',\n",
       " 'while',\n",
       " 'o',\n",
       " 'him',\n",
       " 'didn',\n",
       " 'mightn',\n",
       " 'at',\n",
       " 'shouldn',\n",
       " 'an',\n",
       " 'm',\n",
       " \"doesn't\",\n",
       " 'how',\n",
       " \"hadn't\",\n",
       " 'are',\n",
       " 'll',\n",
       " 'under',\n",
       " 'did',\n",
       " \"weren't\",\n",
       " \"wouldn't\",\n",
       " 'weren',\n",
       " 'some',\n",
       " 'for',\n",
       " 'with',\n",
       " 'shan',\n",
       " 'his',\n",
       " 'both',\n",
       " 'wasn',\n",
       " 'them',\n",
       " 'each',\n",
       " 'then',\n",
       " 'hers',\n",
       " 'only',\n",
       " 'd',\n",
       " 'isn',\n",
       " 'before',\n",
       " 'been',\n",
       " 'have',\n",
       " 'couldn',\n",
       " 'your',\n",
       " 'further',\n",
       " \"hasn't\",\n",
       " 'these',\n",
       " 'does',\n",
       " 's',\n",
       " 'in',\n",
       " 'after',\n",
       " \"you've\",\n",
       " 'yourselves',\n",
       " 've',\n",
       " \"isn't\",\n",
       " 'when',\n",
       " \"you'd\",\n",
       " 'he',\n",
       " 'off',\n",
       " 'i',\n",
       " 'few',\n",
       " 'ain',\n",
       " 'hasn',\n",
       " \"you'll\",\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\"',\n",
       " '\"',\n",
       " '-',\n",
       " '+',\n",
       " '—',\n",
       " 'lt',\n",
       " 'rt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the removal characters such as stopwords and punctuations\n",
    "stop_words = set(stopwords.words('english'))\n",
    "string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—' \n",
    "string.punctuation \n",
    "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt'] \n",
    "removal_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f227da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "# from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bcd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate unigrams, bigrams and trigrams \n",
    "unigram = []\n",
    "bigram = []\n",
    "trigram = []\n",
    "tokenized_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f710399",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sents:\n",
    "    sentence = list(map(lambda x:x.lower(),sentence))\n",
    "    for word in sentence:\n",
    "        if word== '.':\n",
    "            sentence.remove(word)  \n",
    "        else: \n",
    "            unigram.append(word)         \n",
    "    tokenized_text.append(sentence) \n",
    "    bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True))) \n",
    "    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecdc6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the n-grams with removable words \n",
    "def remove_stopwords(x):      \n",
    "    y = [] \n",
    "    for pair in x: \n",
    "        count = 0\n",
    "        for word in pair: \n",
    "            if word in removal_list: \n",
    "                count = count or 0\n",
    "            else: \n",
    "                count = count or 1\n",
    "        if (count==1): \n",
    "            y.append(pair) \n",
    "    return (y) \n",
    "unigram = remove_stopwords(unigram) \n",
    "bigram = remove_stopwords(bigram) \n",
    "trigram = remove_stopwords(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fddf1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency of n-grams  \n",
    "freq_bi = FreqDist(bigram) \n",
    "freq_tri = FreqDist(trigram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40ab865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "d = defaultdict(Counter) \n",
    "for a, b, c in freq_tri:\n",
    "    if a is not None and b is not None and c is not None:\n",
    "        d[a, b] += Counter({c: freq_tri[a, b, c]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f51b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he said\n",
      "he said ,\n",
      "he said , adding\n",
      "he said , adding that\n",
      "he said , adding that maison\n",
      "he said , adding that maison placements\n",
      "he said , adding that maison placements sees\n",
      "he said , adding that maison placements sees inflation\n",
      "he said , adding that maison placements sees inflation rising\n",
      "he said , adding that maison placements sees inflation rising to\n",
      "he said , adding that maison placements sees inflation rising to 462\n",
      "he said , adding that maison placements sees inflation rising to 462 ,\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares ,\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares , or\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares , or one\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares , or one cts\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares , or one cts vs\n",
      "he said , adding that maison placements sees inflation rising to 462 , 000 shares , or one cts vs profit\n"
     ]
    }
   ],
   "source": [
    "# Next word prediction\n",
    "s='' \n",
    "def pick_word(counter): \n",
    "    \"Chooses a random element.\"\n",
    "    return random.choice(list(counter.elements())) \n",
    "prefix = \"he\", \"said\"\n",
    "print(\" \".join(prefix)) \n",
    "s = \" \".join(prefix) \n",
    "for i in range(19): \n",
    "    suffix = pick_word(d[prefix]) \n",
    "    s=s+' '+suffix \n",
    "    print(s) \n",
    "    prefix = prefix[1], suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071cb49",
   "metadata": {},
   "source": [
    "## N-grams examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c31bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I',)\n",
      "('reside',)\n",
      "('in',)\n",
      "('Bengaluru.',)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'I reside in Bengaluru.'\n",
    "n = 1\n",
    "unigrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcbba67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    neutral  \\\n",
      "0   neutral   \n",
      "1  negative   \n",
      "2  positive   \n",
      "3  positive   \n",
      "4  positive   \n",
      "\n",
      "  According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .  \n",
      "0  Technopolis plans to develop in stages an area...                                                                               \n",
      "1  The international electronic industry company ...                                                                               \n",
      "2  With the new production plant the company woul...                                                                               \n",
      "3  According to the company 's updated strategy f...                                                                               \n",
      "4  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...                                                                               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25576\\1028699816.py:4: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(style='seaborn')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "\n",
    "df=pd.read_csv('all-data.csv',encoding = \"ISO-8859-1\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fac58cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral                                                                                                                            0\n",
       "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c794fbed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\personal\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\personal\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\personal\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32mC:\\personal\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\personal\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4a1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
